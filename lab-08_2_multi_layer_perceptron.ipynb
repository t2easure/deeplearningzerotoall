{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37697aaa",
   "metadata": {},
   "source": [
    "## Multilayer perceptron\n",
    "- XOR 문제는 단층 perceptron으로는 해결 불가능.\n",
    "- XOR 문제를 해결하기 위해서는 multilayer perceptron이 필요하다.\n",
    " (MLP)\n",
    "- 여러 개의 층을 갖는 구조 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b74b04",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "- MLP 학습할 수 있음.\n",
    "- 역전파 : 입력 x 가 들어왔을 때 뉴럴 네트워크로 output을 구함. \n",
    "- 역전파 알고리즘 : loss (cost)에 대해서 뉴럴 네트워크에 있는 weight에 대한 미분값을 계산하고 해당 gradient 가지고 뒷 layer부터 해당 loss를 최소화할 수 있도록 weight를 업데이트 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3805c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ab3c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
    "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
    "\n",
    "# nn Layers : nn.Linear 2개 사용했다고 생각하면 됨. \n",
    "w1 = torch.Tensor(2, 2).to(device) # 2개짜리 weight 가짐\n",
    "b1 = torch.Tensor(2).to(device)\n",
    "w2 = torch.Tensor(2, 1).to(device) # 2개 -> 1개 weight 가짐\n",
    "b2 = torch.Tensor(1).to(device)\n",
    "\n",
    "# [수정 후] 정규분포 랜덤 값으로 초기화 (권장)\n",
    "w1 = torch.randn(2, 2).to(device) \n",
    "b1 = torch.randn(2).to(device)\n",
    "w2 = torch.randn(2, 1).to(device)\n",
    "b2 = torch.randn(1).to(device)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + torch.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x): # sigmoid 미분\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38cc56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = torch.add(torch.matmul(X, w1), b1)\n",
    "a1 = sigmoid(l1)\n",
    "l2 = torch.add(torch.matmul(a1, w2), b2)\n",
    "Y_pred = sigmoid(l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35292e5",
   "metadata": {},
   "source": [
    "1. forward propagation\n",
    "$$\\begin{aligned}\n",
    "L_1 &= X W_1 + b_1 \\\\\n",
    "A_1 &= \\sigma(L_1) \\\\\n",
    "L_2 &= A_1 W_2 + b_2 \\\\\n",
    "\\hat{Y} &= \\sigma(L_2)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375bb436",
   "metadata": {},
   "source": [
    "2. BCE (binary cross entropy)\n",
    "$$J = - \\sum \\left[ Y \\log(\\hat{Y}) + (1 - Y) \\log(1 - \\hat{Y}) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc9667",
   "metadata": {},
   "source": [
    "3. backpropagation (chain rule)\n",
    "- Loss Derivative (d_Y_pred): 예측값에 대한 손실함수의 미분$$\\frac{\\partial J}{\\partial \\hat{Y}} = \\frac{\\hat{Y} - Y}{\\hat{Y}(1 - \\hat{Y})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8ef54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c295f83",
   "metadata": {},
   "source": [
    "- Layer 2 Delta (d_l2) : 출력층(Layer 2) 선형 결합 값($l_2$)에 대한 미분 (오차항)$$\\frac{\\partial J}{\\partial L_2} = \\frac{\\partial J}{\\partial \\hat{Y}} \\odot \\sigma'(L_2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c98c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_l2 = d_Y_pred * sigmoid_prime(l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9cee5",
   "metadata": {},
   "source": [
    "- Layer 2 Gradients (d_b2, d_w2)출력층 가중치($W_2$)와 편향($b_2$)에 대한 기울기$$\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial b_2} &= \\frac{\\partial J}{\\partial L_2} \\\\\n",
    "\\frac{\\partial J}{\\partial W_2} &= A_1^T \\cdot \\frac{\\partial J}{\\partial L_2}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b436d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_b2 = d_l2\n",
    "# 두 번째 인자와 세 번째 인자의 차원값을 바꾸기 (교환) :: 10, 5 -> 5, 10ㅇ\n",
    "d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489fa51d",
   "metadata": {},
   "source": [
    "- Layer 1 Activation Gradient (d_a1)은닉층(Layer 1) 출력값($a_1$)으로 역전파된 오차$$\\frac{\\partial J}{\\partial A_1} = \\frac{\\partial J}{\\partial L_2} \\cdot W_2^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2106fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a458cd",
   "metadata": {},
   "source": [
    "- Layer 1 Delta (d_l1)은닉층(Layer 1) 선형 결합 값($l_1$)에 대한 미분$$\\frac{\\partial J}{\\partial L_1} = \\frac{\\partial J}{\\partial A_1} \\odot \\sigma'(L_1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07583c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_l1 = d_a1 * sigmoid_prime(l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8cb14d",
   "metadata": {},
   "source": [
    "- Layer 1 Gradients (d_b1, d_w1)은닉층 가중치($W_1$)와 편향($b_1$)에 대한 기울기$$\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial b_1} &= \\frac{\\partial J}{\\partial L_1} \\\\\n",
    "\\frac{\\partial J}{\\partial W_1} &= X^T \\cdot \\frac{\\partial J}{\\partial L_1}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dfc3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_b1 = d_l1\n",
    "d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bca3b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9613491296768188\n",
      "100 0.13873937726020813\n",
      "200 0.0327175110578537\n",
      "300 0.017057759687304497\n",
      "400 0.011352622881531715\n",
      "500 0.008457552641630173\n",
      "600 0.006719966884702444\n",
      "700 0.005565759725868702\n",
      "800 0.004745173268020153\n",
      "900 0.0041327145881950855\n",
      "1000 0.0036585028283298016\n",
      "1100 0.0032807989045977592\n",
      "1200 0.002972976304590702\n",
      "1300 0.0027174088172614574\n",
      "1400 0.002501869108527899\n",
      "1500 0.00231768935918808\n",
      "1600 0.0021585593931376934\n",
      "1700 0.002019699662923813\n",
      "1800 0.0018974419217556715\n",
      "1900 0.0017890152521431446\n",
      "2000 0.0016922496724873781\n",
      "2100 0.0016052747378125787\n",
      "2200 0.0015267736744135618\n",
      "2300 0.0014555497327819467\n",
      "2400 0.0013905862579122186\n",
      "2500 0.0013311801012605429\n",
      "2600 0.0012765987776219845\n",
      "2700 0.0012262744130566716\n",
      "2800 0.001179698621854186\n",
      "2900 0.001136572565883398\n",
      "3000 0.0010964032262563705\n",
      "3100 0.001059040892869234\n",
      "3200 0.0010240973206236959\n",
      "3300 0.0009913333924487233\n",
      "3400 0.0009606146486476064\n",
      "3500 0.000931702321395278\n",
      "3600 0.0009044915204867721\n",
      "3700 0.0008788480190560222\n",
      "3800 0.000854577636346221\n",
      "3900 0.0008316057501360774\n",
      "4000 0.000809812918305397\n",
      "4100 0.0007891841232776642\n",
      "4200 0.0007695103995501995\n",
      "4300 0.0007508363341912627\n",
      "4400 0.0007330129737965763\n",
      "4500 0.0007160401437431574\n",
      "4600 0.0006997686577960849\n",
      "4700 0.0006842729053460062\n",
      "4800 0.0006694038747809827\n",
      "4900 0.0006552062695845962\n",
      "5000 0.0006416052929125726\n",
      "5100 0.0006285266135819256\n",
      "5200 0.0006159402546472847\n",
      "5300 0.0006038611172698438\n",
      "5400 0.000592289085034281\n",
      "5500 0.0005811199662275612\n",
      "5600 0.0005703237256966531\n",
      "5700 0.0005599898868240416\n",
      "5800 0.0005499991239048541\n",
      "5900 0.0005403664545156062\n",
      "6000 0.000531017140019685\n",
      "6100 0.0005219959421083331\n",
      "6200 0.0005133029189892113\n",
      "6300 0.0005048634484410286\n",
      "6400 0.0004967372515238822\n",
      "6500 0.0004888346884399652\n",
      "6600 0.00048118579434230924\n",
      "6700 0.00047376053407788277\n",
      "6800 0.00046658882638439536\n",
      "6900 0.00045959610724821687\n",
      "7000 0.00045282708015292883\n",
      "7100 0.00044625194277614355\n",
      "7200 0.00043987069511786103\n",
      "7300 0.0004336982383392751\n",
      "7400 0.00042766000842675567\n",
      "7500 0.00042178580770269036\n",
      "7600 0.0004160756361670792\n",
      "7700 0.0004104996914975345\n",
      "7800 0.0004050728748552501\n",
      "7900 0.0003998100874014199\n",
      "8000 0.00039469642797484994\n",
      "8100 0.0003896871639881283\n",
      "8200 0.00038478232454508543\n",
      "8300 0.0003800265258178115\n",
      "8400 0.0003753751516342163\n",
      "8500 0.00037085794610902667\n",
      "8600 0.00036640046164393425\n",
      "8700 0.0003620771167334169\n",
      "8800 0.0003578730975277722\n",
      "8900 0.00035374361323192716\n",
      "9000 0.0003497036232147366\n",
      "9100 0.00034578287159092724\n",
      "9200 0.00034192181192338467\n",
      "9300 0.0003381352871656418\n",
      "9400 0.0003344382275827229\n",
      "9500 0.000330830633174628\n",
      "9600 0.00032729757367633283\n",
      "9700 0.00032383910729549825\n",
      "9800 0.0003204700769856572\n",
      "9900 0.0003171308489982039\n",
      "10000 0.00031389601645059884\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1\n",
    "for step in range(10001):\n",
    "    # forward\n",
    "    l1 = torch.add(torch.matmul(X, w1), b1)\n",
    "    a1 = sigmoid(l1)\n",
    "    l2 = torch.add(torch.matmul(a1, w2), b2)\n",
    "    Y_pred = sigmoid(l2)\n",
    "\n",
    "    # BCE\n",
    "    cost = -torch.mean(Y * torch.log(Y_pred) + (1 - Y) * torch.log(1 - Y_pred))\n",
    "\n",
    "    # back prop (chain rule)\n",
    "    # loss 미분\n",
    "    d_Y_pred = (Y_pred - Y) / (Y_pred * (1.0 - Y_pred) + 1e-7)\n",
    "\n",
    "    # Layer 2\n",
    "    d_l2 = d_Y_pred * sigmoid_prime(l2)\n",
    "    d_b2 = d_l2\n",
    "    d_w2 = torch.matmul(torch.transpose(a1, 0, 1), d_b2)\n",
    "\n",
    "    # Layer 1\n",
    "    d_a1 = torch.matmul(d_b2, torch.transpose(w2, 0, 1)) # layer 2의 오차를 layer 2의 가중치 타고 거꾸로 흘려보내는 과정\n",
    "    d_l1 = d_a1 * sigmoid_prime(l1)\n",
    "    d_b1 = d_l1\n",
    "    d_w1 = torch.matmul(torch.transpose(X, 0, 1), d_b1)\n",
    "\n",
    "    # weight update (gradient descent)\n",
    "    w1 = w1 - learning_rate * d_w1\n",
    "    b1 = b1 - learning_rate * torch.mean(d_b1, 0)\n",
    "    w2 = w2 - learning_rate * d_w2\n",
    "    b2 = b2 - learning_rate * torch.mean(d_b2, 0)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474d960",
   "metadata": {},
   "source": [
    "# XOR - nn\n",
    "- 레이어 2개 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f6e64cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7763229608535767\n",
      "100 0.6930165886878967\n",
      "200 0.692973792552948\n",
      "300 0.6929107308387756\n",
      "400 0.6928110718727112\n",
      "500 0.6926401853561401\n",
      "600 0.6923158764839172\n",
      "700 0.6916120648384094\n",
      "800 0.6897789239883423\n",
      "900 0.6838115453720093\n",
      "1000 0.6620278358459473\n",
      "1100 0.6086074709892273\n",
      "1200 0.48274874687194824\n",
      "1300 0.21770578622817993\n",
      "1400 0.11606816202402115\n",
      "1500 0.07570721954107285\n",
      "1600 0.05526117980480194\n",
      "1700 0.04318012297153473\n",
      "1800 0.03528796508908272\n",
      "1900 0.029761184006929398\n",
      "2000 0.025689953938126564\n",
      "2100 0.02257372811436653\n",
      "2200 0.02011588029563427\n",
      "2300 0.018130186945199966\n",
      "2400 0.016493942588567734\n",
      "2500 0.015123352408409119\n",
      "2600 0.013959166593849659\n",
      "2700 0.012958543375134468\n",
      "2800 0.012089530006051064\n",
      "2900 0.011327981948852539\n",
      "3000 0.010655341669917107\n",
      "3100 0.010056989267468452\n",
      "3200 0.009521331638097763\n",
      "3300 0.009039136581122875\n",
      "3400 0.008602828718721867\n",
      "3500 0.008206162601709366\n",
      "3600 0.007843991741538048\n",
      "3700 0.007512134499847889\n",
      "3800 0.00720686512067914\n",
      "3900 0.00692514143884182\n",
      "4000 0.006664430256932974\n",
      "4100 0.006422386039048433\n",
      "4200 0.006197174079716206\n",
      "4300 0.005987017881125212\n",
      "4400 0.005790514871478081\n",
      "4500 0.005606411024928093\n",
      "4600 0.005433501210063696\n",
      "4700 0.005270846653729677\n",
      "4800 0.005117599852383137\n",
      "4900 0.0049729011952877045\n",
      "5000 0.004836083389818668\n",
      "5100 0.004706534557044506\n",
      "5200 0.00458368519321084\n",
      "5300 0.00446700444445014\n",
      "5400 0.004356138873845339\n",
      "5500 0.004250518511980772\n",
      "5600 0.004149897489696741\n",
      "5700 0.004053935408592224\n",
      "5800 0.003962196875363588\n",
      "5900 0.0038745380006730556\n",
      "6000 0.0037906416691839695\n",
      "6100 0.0037102624773979187\n",
      "6200 0.0036331829614937305\n",
      "6300 0.0035592499189078808\n",
      "6400 0.003488218877464533\n",
      "6500 0.003419931046664715\n",
      "6600 0.0033542802557349205\n",
      "6700 0.003291046479716897\n",
      "6800 0.003230178030207753\n",
      "6900 0.003171483287587762\n",
      "7000 0.003114915918558836\n",
      "7100 0.0030602687038481236\n",
      "7200 0.0030074696987867355\n",
      "7300 0.002956494688987732\n",
      "7400 0.002907192800194025\n",
      "7500 0.002859482541680336\n",
      "7600 0.002813342958688736\n",
      "7700 0.0027686567045748234\n",
      "7800 0.0027253483422100544\n",
      "7900 0.00268333381973207\n",
      "8000 0.002642639447003603\n",
      "8100 0.0026031401939690113\n",
      "8200 0.002564771333709359\n",
      "8300 0.002527517732232809\n",
      "8400 0.002491331659257412\n",
      "8500 0.0024561830796301365\n",
      "8600 0.0024219616316258907\n",
      "8700 0.0023886635899543762\n",
      "8800 0.00235633528791368\n",
      "8900 0.0023248170036822557\n",
      "9000 0.002294158097356558\n",
      "9100 0.002264255890622735\n",
      "9200 0.0022351653315126896\n",
      "9300 0.002206741366535425\n",
      "9400 0.00217910367064178\n",
      "9500 0.002152088563889265\n",
      "9600 0.002125748433172703\n",
      "9700 0.0021000399719923735\n",
      "9800 0.0020749527029693127\n",
      "9900 0.00205045728944242\n",
      "10000 0.0020264973863959312\n"
     ]
    }
   ],
   "source": [
    "linear1 = torch.nn.Linear(2, 2, bias=True)\n",
    "linear2 = torch.nn.Linear(2, 1, bias=True)\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)\n",
    "\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20c5facd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hypothesis:  [[0.00232873]\n",
      " [0.99831223]\n",
      " [0.99831223]\n",
      " [0.00239239]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Accuracy computation\n",
    "# True if hypothesis>0.5 else False\n",
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9ef750",
   "metadata": {},
   "source": [
    "# XOR - nn - wide - deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "375ec0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn layers\n",
    "linear1 = torch.nn.Linear(2, 10, bias=True)\n",
    "linear2 = torch.nn.Linear(10, 10, bias=True)\n",
    "linear3 = torch.nn.Linear(10, 10, bias=True)\n",
    "linear4 = torch.nn.Linear(10, 1, bias=True)\n",
    "sigmoid = torch.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2cf7f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebb4f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss().to(device)\n",
    "optimizer=  torch.optim.SGD(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b3d656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.702433705329895\n",
      "100 0.6930965185165405\n",
      "200 0.6930878162384033\n",
      "300 0.6930782794952393\n",
      "400 0.6930678486824036\n",
      "500 0.6930562257766724\n",
      "600 0.693043053150177\n",
      "700 0.693027913570404\n",
      "800 0.6930105090141296\n",
      "900 0.6929900646209717\n",
      "1000 0.692965567111969\n",
      "1100 0.69293612241745\n",
      "1200 0.6928997039794922\n",
      "1300 0.6928539276123047\n",
      "1400 0.6927951574325562\n",
      "1500 0.6927174925804138\n",
      "1600 0.6926113367080688\n",
      "1700 0.6924604773521423\n",
      "1800 0.6922343373298645\n",
      "1900 0.6918712854385376\n",
      "2000 0.6912301182746887\n",
      "2100 0.6899261474609375\n",
      "2200 0.6865946054458618\n",
      "2300 0.6736891269683838\n",
      "2400 0.5819851160049438\n",
      "2500 0.2226959764957428\n",
      "2600 0.012114075943827629\n",
      "2700 0.005340529605746269\n",
      "2800 0.0032906006090343\n",
      "2900 0.0023381607607007027\n",
      "3000 0.0017971005290746689\n",
      "3100 0.0014514061622321606\n",
      "3200 0.001212847651913762\n",
      "3300 0.0010389507515355945\n",
      "3400 0.0009069546358659863\n",
      "3500 0.0008035794598981738\n",
      "3600 0.0007205012952908874\n",
      "3700 0.000652395945508033\n",
      "3800 0.0005956039531156421\n",
      "3900 0.0005475673242472112\n",
      "4000 0.0005064248689450324\n",
      "4100 0.00047086545964702964\n",
      "4200 0.00043976062443107367\n",
      "4300 0.0004123753169551492\n",
      "4400 0.0003880897129420191\n",
      "4500 0.0003664283431135118\n",
      "4600 0.0003469657967798412\n",
      "4700 0.0003293872287031263\n",
      "4800 0.00031344592571258545\n",
      "4900 0.0002989461936522275\n",
      "5000 0.0002856618957594037\n",
      "5100 0.0002734764711931348\n",
      "5200 0.00026226285262964666\n",
      "5300 0.000251923396717757\n",
      "5400 0.0002422930847387761\n",
      "5500 0.00023336148296948522\n",
      "5600 0.00022506361710838974\n",
      "5700 0.00021731070592068136\n",
      "5800 0.00021004644804634154\n",
      "5900 0.00020327771198935807\n",
      "6000 0.00019686465384438634\n",
      "6100 0.00019084831001237035\n",
      "6200 0.0001852140121627599\n",
      "6300 0.00017984327860176563\n",
      "6400 0.00017481179384049028\n",
      "6500 0.0001700029824860394\n",
      "6600 0.00016546667029615492\n",
      "6700 0.00016115048492792994\n",
      "6800 0.0001570753229316324\n",
      "6900 0.00015315743803512305\n",
      "7000 0.00014945048314984888\n",
      "7100 0.00014588794147130102\n",
      "7200 0.00014249490050133318\n",
      "7300 0.0001392656849930063\n",
      "7400 0.0001361957547487691\n",
      "7500 0.00013319171557668597\n",
      "7600 0.00013036798918619752\n",
      "7700 0.0001276320545002818\n",
      "7800 0.0001250096975127235\n",
      "7900 0.0001224988081958145\n",
      "8000 0.00012006579345325008\n",
      "8100 0.00011773809092119336\n",
      "8200 0.00011551314673852175\n",
      "8300 0.00011332905705785379\n",
      "8400 0.00011124338925583288\n",
      "8500 0.0001092243692255579\n",
      "8600 0.00010729880887083709\n",
      "8700 0.0001054073654813692\n",
      "8800 0.00010360580927226692\n",
      "8900 0.00010183503036387265\n",
      "9000 0.00010015181760536507\n",
      "9100 9.84958023764193e-05\n",
      "9200 9.68954773270525e-05\n",
      "9300 9.537885489407927e-05\n",
      "9400 9.385673911310732e-05\n",
      "9500 9.241618681699038e-05\n",
      "9600 9.102704643737525e-05\n",
      "9700 8.96585188456811e-05\n",
      "9800 8.833940228214487e-05\n",
      "9900 8.700931357452646e-05\n",
      "10000 8.575703395763412e-05\n"
     ]
    }
   ],
   "source": [
    "for step in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    # cost / loss function\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(step, cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886be7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m hypothesis \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m      5\u001b[0m predicted \u001b[38;5;241m=\u001b[39m (hypothesis \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m----> 6\u001b[0m \u001b[43maccuracy\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(    \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHypothesis: \u001b[39m\u001b[38;5;124m'\u001b[39m, hypothesis\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCorrect: \u001b[39m\u001b[38;5;124m'\u001b[39m, predicted\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;124m'\u001b[39m, accuracy\u001b[38;5;241m.\u001b[39mitem()))\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "\n",
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print(    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
